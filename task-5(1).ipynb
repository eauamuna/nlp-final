{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:36:41.005770Z","iopub.execute_input":"2025-05-04T15:36:41.006122Z","iopub.status.idle":"2025-05-04T15:36:44.367492Z","shell.execute_reply.started":"2025-05-04T15:36:41.006016Z","shell.execute_reply":"2025-05-04T15:36:44.366423Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import spacy\n\n# Загрузка модели spaCy\nspacy_nlp = spacy.load(\"en_core_web_sm\")  # Переименовали переменную\n\ntext = \"Barack Obama was born in Hawaii and served as the President of the United States.\"\n\n# POS-разметка с spaCy\ndoc = spacy_nlp(text)\nprint(\"POS tagging with spaCy:\")\nfor token in doc:\n    print(f\"{token.text} - {token.pos_}\")\n\n# NER с spaCy\nprint(\"\\nNER with spaCy:\")\nfor ent in doc.ents:\n    print(f\"{ent.text} - {ent.label_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:45:10.577216Z","iopub.execute_input":"2025-05-04T15:45:10.577563Z","iopub.status.idle":"2025-05-04T15:45:11.338241Z","shell.execute_reply.started":"2025-05-04T15:45:10.577539Z","shell.execute_reply":"2025-05-04T15:45:11.336921Z"}},"outputs":[{"name":"stdout","text":"POS tagging with spaCy:\nBarack - PROPN\nObama - PROPN\nwas - AUX\nborn - VERB\nin - ADP\nHawaii - PROPN\nand - CCONJ\nserved - VERB\nas - ADP\nthe - DET\nPresident - PROPN\nof - ADP\nthe - DET\nUnited - PROPN\nStates - PROPN\n. - PUNCT\n\nNER with spaCy:\nBarack Obama - PERSON\nHawaii - GPE\nthe United States - GPE\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import pipeline\n\n# Создаем отдельную переменную для HuggingFace pipeline\nhf_nlp = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n\n# NER с transformers\nprint(\"\\nNER with transformers:\")\nner_results = hf_nlp(text)\nfor entity in ner_results:\n    print(f\"{entity['word']} - {entity['entity_group']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:45:36.027928Z","iopub.execute_input":"2025-05-04T15:45:36.028297Z","iopub.status.idle":"2025-05-04T15:45:36.489189Z","shell.execute_reply.started":"2025-05-04T15:45:36.028271Z","shell.execute_reply":"2025-05-04T15:45:36.487738Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"\nNER with transformers:\nBarack Obama - PER\nHawaii - LOC\nUnited States - LOC\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")\n\npos_pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\npos_result = pos_pipe(text)\n\nfor word in pos_result:\n    print(f\"{word['word']} - {word['entity_group']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:46:16.445799Z","iopub.execute_input":"2025-05-04T15:46:16.446138Z","iopub.status.idle":"2025-05-04T15:46:20.674136Z","shell.execute_reply.started":"2025-05-04T15:46:16.446115Z","shell.execute_reply":"2025-05-04T15:46:20.672820Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"988a5a455ca1410c89bca20018255224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24bbe14e0fe643b3af3350fdb6d9ec64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2904d3b251b0459696b9cebf1542925e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8bfdc4e83a495ab569b998cf044c9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfcd6f0cdeff419aa248ebaad9d7faba"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"barack obama - PROPN\nwas - AUX\nborn - VERB\nin - ADP\nhawaii - PROPN\nand - CCONJ\nserved - VERB\nas - ADP\nthe - DET\npresident - PROPN\nof - ADP\nthe - DET\nunited states - PROPN\n. - PUNCT\n","output_type":"stream"}],"execution_count":9}]}