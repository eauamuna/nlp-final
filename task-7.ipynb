{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:23:03.919825Z","iopub.execute_input":"2025-05-04T17:23:03.920065Z","iopub.status.idle":"2025-05-04T17:23:03.934692Z","shell.execute_reply.started":"2025-05-04T17:23:03.920043Z","shell.execute_reply":"2025-05-04T17:23:03.933907Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:23:03.935459Z","iopub.execute_input":"2025-05-04T17:23:03.935714Z","iopub.status.idle":"2025-05-04T17:23:03.954231Z","shell.execute_reply.started":"2025-05-04T17:23:03.935690Z","shell.execute_reply":"2025-05-04T17:23:03.953523Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset = load_dataset(\"imdb\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:11:31.785846Z","iopub.execute_input":"2025-05-04T17:11:31.786611Z","iopub.status.idle":"2025-05-04T17:11:36.220265Z","shell.execute_reply.started":"2025-05-04T17:11:31.786590Z","shell.execute_reply":"2025-05-04T17:11:36.219669Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3572001bc8be409b8b97e181c16d7595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af03a552c5bf441297d6f1de9e276df6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0806324f1fa64914abd1f801813dc1da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f2e67fc19354d9b88a5954567ac2a0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f669ebd5022249feb54a1a35b501eef4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78c00bdafa384e53905822ac808bc301"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb956194a7e4851a6dcf0ebba8cca33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a81d1067f840417cba4506db0de28dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a425c76d9414b1288f56ce85e72f3ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03fcf58f750b48eaa4aac71b19b0947b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333090393c4e4303bf8aa3fbff5e3f12"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def tokenize(example):\n    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized = dataset.map(tokenize, batched=True)\ntokenized = tokenized.rename_column(\"label\", \"labels\")\ntokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:11:36.220908Z","iopub.execute_input":"2025-05-04T17:11:36.221192Z","iopub.status.idle":"2025-05-04T17:17:59.430665Z","shell.execute_reply.started":"2025-05-04T17:11:36.221148Z","shell.execute_reply":"2025-05-04T17:17:59.430032Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c97835b75d4b4a9b62af8aa5b376ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3980d8925b0a4066825213d9173c81ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff0c2f490ef41149d5f640feaabb21f"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nmodel.to(device)\n\nargs = TrainingArguments(\n    output_dir=\"./bert_sentiment\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=1,\n    save_strategy=\"no\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:18:33.249955Z","iopub.execute_input":"2025-05-04T17:18:33.250258Z","iopub.status.idle":"2025-05-04T17:18:34.057576Z","shell.execute_reply.started":"2025-05-04T17:18:33.250236Z","shell.execute_reply":"2025-05-04T17:18:34.056976Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    acc = accuracy_score(p.label_ids, preds)\n    f1 = f1_score(p.label_ids, preds)\n    return {\"accuracy\": acc, \"f1\": f1}\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized[\"train\"].shuffle(seed=42).select(range(2000)),\n    eval_dataset=tokenized[\"test\"].select(range(1000)),\n    compute_metrics=compute_metrics\n)\n\n# Обучение\ntrainer.train()\nbert_results = trainer.evaluate()\nprint(\"BERT results:\", bert_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:23:13.261202Z","iopub.execute_input":"2025-05-04T17:23:13.261461Z","iopub.status.idle":"2025-05-04T17:25:25.752040Z","shell.execute_reply.started":"2025-05-04T17:23:13.261443Z","shell.execute_reply":"2025-05-04T17:25:25.751209Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 01:52, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.450200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.278200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.248400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.121800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.156900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.262300</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.357800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.262800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.211600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.190100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.256100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.174500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"BERT results: {'eval_loss': 0.27451398968696594, 'eval_accuracy': 0.896, 'eval_f1': 0.0, 'eval_runtime': 18.091, 'eval_samples_per_second': 55.276, 'eval_steps_per_second': 3.482, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class IMDbDataset(Dataset):\n    def __init__(self, texts, labels, vectorizer):\n        self.X = torch.tensor(vectorizer.transform(texts).toarray(), dtype=torch.float32)\n        self.y = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Пример: 1000 примеров для обучения\ntexts = dataset[\"train\"][\"text\"][:1000]\nlabels = dataset[\"train\"][\"label\"][:1000]\ntexts_test = dataset[\"test\"][\"text\"][:200]\nlabels_test = dataset[\"test\"][\"label\"][:200]\n\nvectorizer = CountVectorizer(max_features=2000)\nvectorizer.fit(texts)\n\ntrain_data = IMDbDataset(texts, labels, vectorizer)\ntest_data = IMDbDataset(texts_test, labels_test, vectorizer)\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:28:43.906750Z","iopub.execute_input":"2025-05-04T17:28:43.907407Z","iopub.status.idle":"2025-05-04T17:28:44.385562Z","shell.execute_reply.started":"2025-05-04T17:28:43.907374Z","shell.execute_reply":"2025-05-04T17:28:44.384807Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class LSTMSentiment(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # add sequence dimension\n        _, (h_n, _) = self.lstm(x)\n        return self.fc(h_n.squeeze(0))\n\nmodel = LSTMSentiment(2000, 128, 2)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Обучение LSTM\nfor epoch in range(1):\n    model.train()\n    for x_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        output = model(x_batch)\n        loss = criterion(output, y_batch)\n        loss.backward()\n        optimizer.step()\n\n# Оценка\nmodel.eval()\nall_preds = []\nall_labels = []\nwith torch.no_grad():\n    for x_batch, y_batch in test_loader:\n        output = model(x_batch)\n        preds = torch.argmax(output, dim=1)\n        all_preds.extend(preds.tolist())\n        all_labels.extend(y_batch.tolist())\n\nfrom sklearn.metrics import accuracy_score, f1_score\nprint(\"LSTM Accuracy:\", accuracy_score(all_labels, all_preds))\nprint(\"LSTM F1:\", f1_score(all_labels, all_preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T17:28:48.289520Z","iopub.execute_input":"2025-05-04T17:28:48.289769Z","iopub.status.idle":"2025-05-04T17:28:48.859915Z","shell.execute_reply.started":"2025-05-04T17:28:48.289750Z","shell.execute_reply":"2025-05-04T17:28:48.859013Z"}},"outputs":[{"name":"stdout","text":"LSTM Accuracy: 1.0\nLSTM F1: 0.0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}