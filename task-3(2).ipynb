{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:19:57.996525Z","iopub.execute_input":"2025-05-04T15:19:57.996822Z","iopub.status.idle":"2025-05-04T15:19:58.306311Z","shell.execute_reply.started":"2025-05-04T15:19:57.996793Z","shell.execute_reply":"2025-05-04T15:19:58.305473Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:19:58.307197Z","iopub.execute_input":"2025-05-04T15:19:58.307529Z","iopub.status.idle":"2025-05-04T15:20:00.001676Z","shell.execute_reply.started":"2025-05-04T15:19:58.307510Z","shell.execute_reply":"2025-05-04T15:20:00.001035Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=2100):  # Increased buffer\n        super().__init__()\n        self.d_model = d_model\n        self.max_len = max_len\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        if seq_len > self.max_len:\n            # Instead of error, automatically extend (safer than dynamic)\n            warnings.warn(f\"Extending positional encodings from {self.max_len} to {seq_len}\")\n            self.extend_pe(seq_len)\n        return x + self.pe[:, :seq_len].to(x.device)\n    \n    def extend_pe(self, new_max_len):\n        \"\"\"Extend positional embeddings if needed\"\"\"\n        if new_max_len <= self.max_len:\n            return\n            \n        new_pe = torch.zeros(new_max_len, self.d_model, device=self.pe.device)\n        new_pe[:self.max_len] = self.pe[0]\n        \n        position = torch.arange(self.max_len, new_max_len, device=self.pe.device).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, self.d_model, 2, device=self.pe.device) * \n                          -(math.log(10000.0) / self.d_model))\n        new_pe[self.max_len:, 0::2] = torch.sin(position * div_term)\n        new_pe[self.max_len:, 1::2] = torch.cos(position * div_term)\n        \n        self.pe = new_pe.unsqueeze(0)\n        self.max_len = new_max_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:00.002432Z","iopub.execute_input":"2025-05-04T15:20:00.002798Z","iopub.status.idle":"2025-05-04T15:20:00.012042Z","shell.execute_reply.started":"2025-05-04T15:20:00.002769Z","shell.execute_reply":"2025-05-04T15:20:00.011334Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        self.d_k = d_model // num_heads\n        self.num_heads = num_heads\n\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.out = nn.Linear(d_model, d_model)\n\n    def forward(self, q, k, v):\n        bs = q.size(0)\n\n        # линейные преобразования и деление на головы\n        def split(x): return x.view(bs, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n        q, k, v = map(split, (self.q_linear(q), self.k_linear(k), self.v_linear(v)))\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        attn = F.softmax(scores, dim=-1)\n        x = torch.matmul(attn, v)\n\n        x = x.transpose(1, 2).contiguous().view(bs, -1, self.num_heads * self.d_k)\n        return self.out(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:00.012734Z","iopub.execute_input":"2025-05-04T15:20:00.012904Z","iopub.status.idle":"2025-05-04T15:20:00.039393Z","shell.execute_reply.started":"2025-05-04T15:20:00.012890Z","shell.execute_reply":"2025-05-04T15:20:00.038861Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:00.039987Z","iopub.execute_input":"2025-05-04T15:20:00.040219Z","iopub.status.idle":"2025-05-04T15:20:00.066595Z","shell.execute_reply.started":"2025-05-04T15:20:00.040204Z","shell.execute_reply":"2025-05-04T15:20:00.065994Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        attn_out = self.self_attn(x, x, x)\n        x = self.norm1(x + attn_out)\n        ff_out = self.feed_forward(x)\n        return self.norm2(x + ff_out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:00.067212Z","iopub.execute_input":"2025-05-04T15:20:00.067442Z","iopub.status.idle":"2025-05-04T15:20:00.083218Z","shell.execute_reply.started":"2025-05-04T15:20:00.067426Z","shell.execute_reply":"2025-05-04T15:20:00.082774Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, num_classes=2, max_len=2100):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pos_enc = PositionalEncoding(d_model, max_len)\n        self.layers = nn.ModuleList([\n            TransformerEncoderLayer(d_model, num_heads, d_ff=512)\n            for _ in range(num_layers)\n        ])\n        self.classifier = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.embed(x)\n        x = self.pos_enc(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = x.mean(dim=1)  # Global Average Pooling\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:00.085618Z","iopub.execute_input":"2025-05-04T15:20:00.085790Z","iopub.status.idle":"2025-05-04T15:20:00.102753Z","shell.execute_reply.started":"2025-05-04T15:20:00.085777Z","shell.execute_reply":"2025-05-04T15:20:00.102066Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n# 2. Verify imports work\nimport torch\nfrom torchtext.datasets import IMDB\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"IMDB dataset loaded successfully\")\n\n# 3. Alternative if you need newer versions\n# !pip install torch==2.1.0 torchtext==0.16.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:00.103466Z","iopub.execute_input":"2025-05-04T15:20:00.103731Z","iopub.status.idle":"2025-05-04T15:20:02.136567Z","shell.execute_reply.started":"2025-05-04T15:20:00.103708Z","shell.execute_reply":"2025-05-04T15:20:02.135704Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.1.0+cu121\nIMDB dataset loaded successfully\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchtext.datasets import IMDB\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.nn.utils.rnn import pad_sequence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:02.137399Z","iopub.execute_input":"2025-05-04T15:20:02.137990Z","iopub.status.idle":"2025-05-04T15:20:02.142889Z","shell.execute_reply.started":"2025-05-04T15:20:02.137969Z","shell.execute_reply":"2025-05-04T15:20:02.142038Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"tokenizer = get_tokenizer(\"basic_english\")\n\ndef yield_tokens(data_iter):\n    for label, line in data_iter:\n        yield tokenizer(line)\n\ntrain_iter = IMDB(split='train')\nvocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<pad>\", \"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\n# Перезагрузим после итерирования\ntrain_iter = IMDB(split='train')\ntest_iter = IMDB(split='test')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:02.143700Z","iopub.execute_input":"2025-05-04T15:20:02.143904Z","iopub.status.idle":"2025-05-04T15:20:20.922520Z","shell.execute_reply.started":"2025-05-04T15:20:02.143888Z","shell.execute_reply":"2025-05-04T15:20:20.921688Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"text_pipeline = lambda x: vocab(tokenizer(x))\nlabel_pipeline = lambda x: 1 if x == \"pos\" else 0\n\ndef collate_batch(batch):\n    label_list, text_list = [], []\n    max_len = 2100  # Hard truncation to match model's max_len\n    for label, text in batch:\n        label_list.append(label_pipeline(label))\n        processed_text = text_pipeline(text)[:max_len]  # Trim to max_len\n        text_list.append(torch.tensor(processed_text, dtype=torch.long))\n    text_list = pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n    return text_list.to(device), torch.tensor(label_list, dtype=torch.long).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:20.923555Z","iopub.execute_input":"2025-05-04T15:20:20.923772Z","iopub.status.idle":"2025-05-04T15:20:20.928614Z","shell.execute_reply.started":"2025-05-04T15:20:20.923757Z","shell.execute_reply":"2025-05-04T15:20:20.927808Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"batch_size = 16\ntrain_loader = DataLoader(list(IMDB(split='train')), batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\ntest_loader = DataLoader(list(IMDB(split='test')), batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:20.929369Z","iopub.execute_input":"2025-05-04T15:20:20.929631Z","iopub.status.idle":"2025-05-04T15:20:30.271385Z","shell.execute_reply.started":"2025-05-04T15:20:20.929615Z","shell.execute_reply":"2025-05-04T15:20:30.270545Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = TransformerClassifier(\n    vocab_size=len(vocab),\n    d_model=128,\n    num_heads=4,\n    num_layers=2,\n    num_classes=2,\n    max_len=2100\n).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:30.272394Z","iopub.execute_input":"2025-05-04T15:20:30.272605Z","iopub.status.idle":"2025-05-04T15:20:30.703728Z","shell.execute_reply.started":"2025-05-04T15:20:30.272590Z","shell.execute_reply":"2025-05-04T15:20:30.703119Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss()\nscaler = GradScaler()\n\ndef train_epoch(model, dataloader):\n    model.train()\n    total_loss, correct = 0, 0\n    for x, y in dataloader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        with autocast():  # Enable mixed precision\n            out = model(x)\n            loss = criterion(out, y)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        total_loss += loss.item()\n        correct += (out.argmax(1) == y).sum().item()\n    return total_loss / len(dataloader), correct / len(dataloader.dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:30.704388Z","iopub.execute_input":"2025-05-04T15:20:30.704573Z","iopub.status.idle":"2025-05-04T15:20:32.562162Z","shell.execute_reply.started":"2025-05-04T15:20:30.704559Z","shell.execute_reply":"2025-05-04T15:20:32.561625Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def evaluate(model, dataloader):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for x, y in dataloader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            correct += (out.argmax(1) == y).sum().item()\n    return correct / len(dataloader.dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:32.562792Z","iopub.execute_input":"2025-05-04T15:20:32.563079Z","iopub.status.idle":"2025-05-04T15:20:32.567193Z","shell.execute_reply.started":"2025-05-04T15:20:32.563064Z","shell.execute_reply":"2025-05-04T15:20:32.566580Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"for epoch in range(5):\n    train_loss, train_acc = train_epoch(model, train_loader)\n    test_acc = evaluate(model, test_loader)\n    print(f\"Epoch {epoch+1}: Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Test Acc={test_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:20:32.568702Z","iopub.execute_input":"2025-05-04T15:20:32.568971Z","iopub.status.idle":"2025-05-04T15:28:38.121768Z","shell.execute_reply.started":"2025-05-04T15:20:32.568950Z","shell.execute_reply":"2025-05-04T15:28:38.120755Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Loss=0.0048, Train Acc=0.9976, Test Acc=1.0000\nEpoch 2: Loss=0.0001, Train Acc=1.0000, Test Acc=1.0000\nEpoch 3: Loss=0.0000, Train Acc=1.0000, Test Acc=1.0000\nEpoch 4: Loss=0.0000, Train Acc=1.0000, Test Acc=1.0000\nEpoch 5: Loss=0.0000, Train Acc=1.0000, Test Acc=1.0000\n","output_type":"stream"}],"execution_count":16}]}